---
title: chapter 10.2.7
author: minsu
date: 2023-03-14 01:36:00 +0900
categories: [HandsOnML, Chapter-10]
tags: ["ML","Perceptron","Visualization"]
---

# 10.2.7 콜백 사용하기

* fit() 메서드의 callbacks 매개변수를 사용하여 케라스가 훈련의 시작이나 끝에 호출할 객체 리스트를 지정할 수 있습니다.
* 또한 에포크의 시작이나 끝, 각 배치 처리 전 후에 호출할 수도 있습니다.

* 예를 들어 ModelCheckpoint는 훈련하는 동안 일정한 간격으로 모델의 체크포인트를 저장합니다.
* 기본적으로 매 에포크의 끝에서 호출합니다.


```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow import keras

housing = fetch_california_housing()

X_train_full, X_test, y_train_full, y_test = train_test_split(
    housing.data, housing.target
)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full
)

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)

# A, B 두개로 분리
X_train_A, X_train_B = X_train[:,:5], X_train[:,2:]
X_valid_A, X_valid_B = X_valid[:,:5], X_valid[:,2:]
X_test_A, X_test_B = X_test[:,:5],X_test[:,2:]
X_new_A,X_new_B = X_test_A[:3], X_test_B[:3]

## 출력층 까지는 이전과 동일하다.
# 모델 설정
input_ = keras.layers.Input(shape = X_train.shape[1:])
hidden1 = keras.layers.Dense(30, activation = 'relu')(input_)
hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)
concat = keras.layers.Concatenate()([input_, hidden2])
output = keras.layers.Dense(1)(concat)
model = keras.Model(inputs = [input_], outputs = [output])
# 모델 컴파일
model.compile(loss='mse', optimizer = keras.optimizers.SGD(lr=1e-3))

# checkpoint 만들기 
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5")
history = model.fit(X_train, y_train, epochs = 10, callbacks = [checkpoint_cb])
```

    WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.


    Epoch 1/10
    363/363 [==============================] - 0s 329us/step - loss: 0.8373
    Epoch 2/10
    363/363 [==============================] - 0s 308us/step - loss: 3.7780
    Epoch 3/10
    363/363 [==============================] - 0s 308us/step - loss: 0.5277
    Epoch 4/10
    363/363 [==============================] - 0s 350us/step - loss: 0.5546
    Epoch 5/10
    363/363 [==============================] - 0s 306us/step - loss: 33.3387
    Epoch 6/10
    363/363 [==============================] - 0s 307us/step - loss: 0.5314
    Epoch 7/10
    363/363 [==============================] - 0s 305us/step - loss: 0.4380
    Epoch 8/10
    363/363 [==============================] - 0s 308us/step - loss: 0.4259
    Epoch 9/10
    363/363 [==============================] - 0s 307us/step - loss: 0.4125
    Epoch 10/10
    363/363 [==============================] - 0s 306us/step - loss: 0.4038


* save_best_only =True 지정할 수 있다.
    * 이렇게 하면 최상의 검증 세트 점수에서만 모델을 저장한다.



```python
checkpoint_cb = keras.callbacks.ModelCheckpoint("my_keras_model.h5",
                                                save_best_only=True)

model.summary()
history = model.fit(X_train,y_train, epochs = 10,
                    validation_data = (X_valid, y_valid),
                    callbacks = [checkpoint_cb])
model = keras.models.load_model("my_keras_model.h5")

model.summary()
```

    Model: "model_3"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     input_2 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                      
     dense_9 (Dense)                (None, 30)           270         ['input_2[0][0]']                
                                                                                                      
     dense_10 (Dense)               (None, 30)           930         ['dense_9[0][0]']                
                                                                                                      
     concatenate_3 (Concatenate)    (None, 38)           0           ['input_2[0][0]',                
                                                                      'dense_10[0][0]']               
                                                                                                      
     dense_11 (Dense)               (None, 1)            39          ['concatenate_3[0][0]']          
                                                                                                      
    ==================================================================================================
    Total params: 1,239
    Trainable params: 1,239
    Non-trainable params: 0
    __________________________________________________________________________________________________
    Epoch 1/10
    363/363 [==============================] - 0s 596us/step - loss: 0.4008 - val_loss: 0.3790
    Epoch 2/10
    363/363 [==============================] - 0s 413us/step - loss: 0.3990 - val_loss: 0.3642
    Epoch 3/10
    363/363 [==============================] - 0s 408us/step - loss: 0.3941 - val_loss: 0.3575
    Epoch 4/10
    363/363 [==============================] - 0s 397us/step - loss: 0.3832 - val_loss: 0.3602
    Epoch 5/10
    363/363 [==============================] - 0s 412us/step - loss: 0.5403 - val_loss: 0.3260
    Epoch 6/10
    363/363 [==============================] - 0s 398us/step - loss: 0.3911 - val_loss: 0.3315
    Epoch 7/10
    363/363 [==============================] - 0s 399us/step - loss: 0.5801 - val_loss: 0.4043
    Epoch 8/10
    363/363 [==============================] - 0s 396us/step - loss: 0.8301 - val_loss: 0.3370
    Epoch 9/10
    363/363 [==============================] - 0s 396us/step - loss: 0.3724 - val_loss: 0.3301
    Epoch 10/10
    363/363 [==============================] - 0s 394us/step - loss: 0.3846 - val_loss: 0.3585
    Model: "model_3"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     input_2 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                      
     dense_9 (Dense)                (None, 30)           270         ['input_2[0][0]']                
                                                                                                      
     dense_10 (Dense)               (None, 30)           930         ['dense_9[0][0]']                
                                                                                                      
     concatenate_3 (Concatenate)    (None, 38)           0           ['input_2[0][0]',                
                                                                      'dense_10[0][0]']               
                                                                                                      
     dense_11 (Dense)               (None, 1)            39          ['concatenate_3[0][0]']          
                                                                                                      
    ==================================================================================================
    Total params: 1,239
    Trainable params: 1,239
    Non-trainable params: 0
    __________________________________________________________________________________________________


## 조기 종료를 구현하는 방법

* EarlyStopping 콜백을 사용 하는 방법
    * 일정 에포크 동안 검증 세트에 대한 점수가 상향되지 않으면 훈련을 멈춤니다.


```python
early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10,
                                                  restore_best_weights = True)

history = model.fit(X_train, y_train, epochs = 100,
                    validation_data = (X_valid, y_valid),
                    callbacks = [checkpoint_cb, early_stopping_cb])
```

    Epoch 1/100
    363/363 [==============================] - 0s 554us/step - loss: 0.3671 - val_loss: 0.3581
    Epoch 2/100
    363/363 [==============================] - 0s 431us/step - loss: 0.6158 - val_loss: 0.3368
    Epoch 3/100
    363/363 [==============================] - 0s 506us/step - loss: 0.3607 - val_loss: 0.3666
    Epoch 4/100
    363/363 [==============================] - 0s 439us/step - loss: 0.3717 - val_loss: 0.3624
    Epoch 5/100
    363/363 [==============================] - 0s 417us/step - loss: 0.3647 - val_loss: 0.3701
    Epoch 6/100
    363/363 [==============================] - 0s 443us/step - loss: 0.7197 - val_loss: 0.3147
    Epoch 7/100
    363/363 [==============================] - 0s 420us/step - loss: 0.3600 - val_loss: 0.3164
    Epoch 8/100
    363/363 [==============================] - 0s 411us/step - loss: 0.3436 - val_loss: 0.3634
    Epoch 9/100
    363/363 [==============================] - 0s 421us/step - loss: 0.3625 - val_loss: 0.3731
    Epoch 10/100
    363/363 [==============================] - 0s 414us/step - loss: 0.3616 - val_loss: 0.3450
    Epoch 11/100
    363/363 [==============================] - 0s 432us/step - loss: 0.3531 - val_loss: 0.3544
    Epoch 12/100
    363/363 [==============================] - 0s 427us/step - loss: 0.6718 - val_loss: 0.3143
    Epoch 13/100
    363/363 [==============================] - 0s 415us/step - loss: 0.3500 - val_loss: 0.3145
    Epoch 14/100
    363/363 [==============================] - 0s 411us/step - loss: 0.3603 - val_loss: 0.3101
    Epoch 15/100
    363/363 [==============================] - 0s 395us/step - loss: 0.3322 - val_loss: 0.3532
    Epoch 16/100
    363/363 [==============================] - 0s 409us/step - loss: 0.6483 - val_loss: 0.3028
    Epoch 17/100
    363/363 [==============================] - 0s 425us/step - loss: 0.3322 - val_loss: 0.3813
    Epoch 18/100
    363/363 [==============================] - 0s 394us/step - loss: 0.3540 - val_loss: 0.3681
    Epoch 19/100
    363/363 [==============================] - 0s 396us/step - loss: 0.3475 - val_loss: 0.3658
    Epoch 20/100
    363/363 [==============================] - 0s 427us/step - loss: 0.7895 - val_loss: 0.2981
    Epoch 21/100
    363/363 [==============================] - 0s 413us/step - loss: 0.3474 - val_loss: 0.2958
    Epoch 22/100
    363/363 [==============================] - 0s 409us/step - loss: 0.3267 - val_loss: 0.3697
    Epoch 23/100
    363/363 [==============================] - 0s 395us/step - loss: 0.3501 - val_loss: 0.3902
    Epoch 24/100
    363/363 [==============================] - 0s 399us/step - loss: 0.8028 - val_loss: 0.3015
    Epoch 25/100
    363/363 [==============================] - 0s 400us/step - loss: 0.3226 - val_loss: 0.4125
    Epoch 26/100
    363/363 [==============================] - 0s 394us/step - loss: 1.0977 - val_loss: 0.3491
    Epoch 27/100
    363/363 [==============================] - 0s 399us/step - loss: 0.3538 - val_loss: 0.3074
    Epoch 28/100
    363/363 [==============================] - 0s 400us/step - loss: 0.3267 - val_loss: 0.4141
    Epoch 29/100
    363/363 [==============================] - 0s 429us/step - loss: 0.3571 - val_loss: 0.3876
    Epoch 30/100
    363/363 [==============================] - 0s 393us/step - loss: 0.9295 - val_loss: 0.3035
    Epoch 31/100
    363/363 [==============================] - 0s 392us/step - loss: 0.3529 - val_loss: 0.3127



```python
# EarlyStooping 콜백이 훈련이 끝난 후 최상의 가중치를 복원하기 떄문에 저장 된 모델을 따로 복원할 필요가 없다.

import pandas as pd
import matplotlib.pyplot as plt

pd.DataFrame(history.history).plot(figsize = (8,5))
plt.grid(True)
plt.gca().set_ylim(0,1)
plt.show()
```


    
![png](_posts/10.2.7_files/10.2.7_8_0.png)
    